{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameter Configuration\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation of Dataset and DataLoader\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define paths to data directories\n",
    "root_dir = \"/content/drive/MyDrive/음원 mel-spectrogram 7초짜리 복사본\"\n",
    "train_dir = os.path.join(root_dir, \"train\")\n",
    "val_dir = os.path.join(root_dir, \"val\")\n",
    "\n",
    "# Define transforms for data preprocessing\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# Load the full dataset\n",
    "dataset = ImageFolder(root=root_dir, transform=transform_train)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "num_samples = len(dataset)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(0.2 * num_samples))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "# Create data loaders for training and validation sets\n",
    "batch_size = 64\n",
    "num_workers = 4  # number of processes to use for data loading\n",
    "pin_memory = True  # whether to use pinned memory for GPU acceleration\n",
    "print(\"Creating data loaders...\")\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "val_dataloader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=num_workers, pin_memory=pin_memory)\n",
    "print(\"Data loaders created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# 모델 구성\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mCNN\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      3\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m      4\u001b[0m         \u001b[39msuper\u001b[39m(CNN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# Model Configuration\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(in_features=64 * 30 * 30, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation and Optimization Function Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path where to save the checkpoint\n",
    "checkpoint_dir = \"/content/drive/MyDrive/checkpoints/7초짜리\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "best_accuracy = 0.0\n",
    "best_f1 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import display, Javascript\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Display the message \"Runtime is still alive\" every 30 minutes\n",
    "# Send a request to maintain the runtime\n",
    "display(Javascript('''\n",
    " function ClickConnect(){\n",
    "   console.log(\"Working\"); \n",
    "   document.querySelector(\"colab-toolbar-button#connect\").click() \n",
    "}\n",
    "setInterval(ClickConnect,1000*60*30);\n",
    "'''))\n",
    "# Define the path where to save the checkpoint\n",
    "checkpoint_path = '/content/drive/MyDrive/진짜 딥러닝 진행 세이브.pth'\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "best_accuracy = 0.0\n",
    "best_f1_score = 0.0 # 추가\n",
    "best_precision = 0.0\n",
    "best_recall = 0.0\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training loop\n",
    "    model.train() # Set model to training mode\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Print progress every 10 batches\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_dataloader)}], Loss: {loss.item()}\")\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        val_loss = 0.0\n",
    "        y_true, y_pred = [], []\n",
    "        for i, (images, labels) in enumerate(val_dataloader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            # Print progress every 10 batches\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Validation: Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(val_dataloader)}], Loss: {loss.item()}, Accuracy: {(predicted == labels).sum().item() / labels.size(0)}\")\n",
    "        accuracy = correct / total\n",
    "        val_loss /= len(val_dataloader)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "        precision = precision_score(y_true, y_pred, average='weighted')\n",
    "        recall = recall_score(y_true, y_pred, average='weighted')\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Validation Loss: {val_loss}, Accuracy: {accuracy}, F1-score: {f1}, Precision: {precision}, Recall: {recall}\")\n",
    "\n",
    "        \n",
    "        # Save the best model checkpoint based on validation accuracy and F1-score\n",
    "        if accuracy > best_accuracy or f1 > best_f1_score or precision > best_precision or recall > best_recall:\n",
    "            if f1 > best_f1_score:\n",
    "                print(f\"F1-score improved from {best_f1_score:.4f} to {f1:.4f}\")\n",
    "                best_f1_score = f1\n",
    "            if accuracy > best_accuracy:\n",
    "                print(f\"Accuracy improved from {best_accuracy:.4f} to {accuracy:.4f}\")\n",
    "                best_accuracy = accuracy\n",
    "            if precision > best_precision:\n",
    "                print(f\"Precision improved from {best_precision:.4f} to {precision:.4f}\")\n",
    "            if recall > best_recall:\n",
    "                print(f\"Recall improved from {best_recall:.4f} to {recall:.4f}\")\n",
    "                best_recall = recall\n",
    "    \n",
    "    # Save the model checkpoint after each epoch\n",
    "    checkpoint_path = f\"music_classifier_epoch{epoch+1}.pth\"\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# Save the final trained model\n",
    "checkpoint_path = os.path.join(checkpoint_dir, \"music_classifier_final.pth\")\n",
    "torch.save(model.state_dict(), \"music_classifier.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the stored checkpoint\n",
    "checkpoint_path = '/content/gdrive/MyDrive/music_classifier.pth'\n",
    "model.load_state_dict(torch.load(checkpoint_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5f72433a7824617cd4cd276e827ec1f07f34627b05c38560b2f3f65ac2f5f29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
